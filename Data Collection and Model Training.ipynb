{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import gensim\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use the NLTK downloader to download stopwords and punkt tokenizer (for breaking paragraphs into sentences)\n",
    "\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "database.sqlite\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sql_conn = sqlite3.connect('../data/database.sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mathematics = pd.read_sql(\"SELECT subreddit, body FROM May2015 WHERE subreddit == 'mathematics'\",sql_conn)\n",
    "\n",
    "computerscience = pd.read_sql(\"SELECT subreddit, body FROM May2015 WHERE subreddit == 'computerscience'\",sql_conn)\n",
    "\n",
    "history = pd.read_sql(\"SELECT subreddit, body FROM May2015 WHERE subreddit == 'history'\",sql_conn)\n",
    "\n",
    "philosophy = pd.read_sql(\"SELECT subreddit, body FROM May2015 WHERE subreddit == 'philosophy'\",sql_conn)\n",
    "\n",
    "elifive = pd.read_sql(\"SELECT subreddit, body FROM May2015 WHERE subreddit == 'explainlikeimfive'\",sql_conn)\n",
    "\n",
    "askanthro = pd.read_sql(\"SELECT subreddit, body FROM May2015 WHERE subreddit == 'AskAnthropology'\",sql_conn)\n",
    "\n",
    "homebrewing = pd.read_sql(\"SELECT subreddit, body FROM May2015 WHERE subreddit == 'Homebrewing'\",sql_conn)\n",
    "\n",
    "bicycling = pd.read_sql(\"SELECT subreddit, body FROM May2015 WHERE subreddit == 'bicycling'\", sql_conn)\n",
    "\n",
    "food = pd.read_sql(\"SELECT subreddit, body FROM May2015 WHERE subreddit == 'food'\", sql_conn)\n",
    "\n",
    "science = pd.read_sql(\"SELECT subreddit, body FROM May2015 WHERE subreddit == 'science'\", sql_conn)\n",
    "\n",
    "movies = pd.read_sql(\"SELECT subreddit, body FROM May2015 WHERE subreddit == 'movies'\", sql_conn)\n",
    "\n",
    "books = pd.read_sql(\"SELECT subreddit, body FROM May2015 WHERE subreddit == 'books'\", sql_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Array of tuples, with df and subject\n",
    "subreddits = [(bicycling,'bicycling'),(history,'history'),(philosophy,'philosophy'),\n",
    "              (elifive,'explain'),(homebrewing,'homebrew'),(askanthro,'anthropology'),\n",
    "              (mathematics,'mathematics'),(computerscience,'computer science'),\n",
    "              (food,'food'),(science,'science'),(movies,'movies'),(books,'books')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bicycling\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 40111 entries, 0 to 40110\n",
      "Data columns (total 2 columns):\n",
      "subreddit    40111 non-null object\n",
      "body         40111 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 940.1+ KB\n",
      "None\n",
      "=========\n",
      "\n",
      "history\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 25242 entries, 0 to 25241\n",
      "Data columns (total 2 columns):\n",
      "subreddit    25242 non-null object\n",
      "body         25242 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 591.6+ KB\n",
      "None\n",
      "=========\n",
      "\n",
      "philosophy\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 16943 entries, 0 to 16942\n",
      "Data columns (total 2 columns):\n",
      "subreddit    16943 non-null object\n",
      "body         16943 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 397.1+ KB\n",
      "None\n",
      "=========\n",
      "\n",
      "explain\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 223148 entries, 0 to 223147\n",
      "Data columns (total 2 columns):\n",
      "subreddit    223148 non-null object\n",
      "body         223148 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 5.1+ MB\n",
      "None\n",
      "=========\n",
      "\n",
      "homebrew\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 29948 entries, 0 to 29947\n",
      "Data columns (total 2 columns):\n",
      "subreddit    29948 non-null object\n",
      "body         29948 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 701.9+ KB\n",
      "None\n",
      "=========\n",
      "\n",
      "anthropology\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1624 entries, 0 to 1623\n",
      "Data columns (total 2 columns):\n",
      "subreddit    1624 non-null object\n",
      "body         1624 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 38.1+ KB\n",
      "None\n",
      "=========\n",
      "\n",
      "mathematics\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 150 entries, 0 to 149\n",
      "Data columns (total 2 columns):\n",
      "subreddit    150 non-null object\n",
      "body         150 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 3.5+ KB\n",
      "None\n",
      "=========\n",
      "\n",
      "computer science\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 711 entries, 0 to 710\n",
      "Data columns (total 2 columns):\n",
      "subreddit    711 non-null object\n",
      "body         711 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 16.7+ KB\n",
      "None\n",
      "=========\n",
      "\n",
      "food\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 55231 entries, 0 to 55230\n",
      "Data columns (total 2 columns):\n",
      "subreddit    55231 non-null object\n",
      "body         55231 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.3+ MB\n",
      "None\n",
      "=========\n",
      "\n",
      "science\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 89413 entries, 0 to 89412\n",
      "Data columns (total 2 columns):\n",
      "subreddit    89413 non-null object\n",
      "body         89413 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 2.0+ MB\n",
      "None\n",
      "=========\n",
      "\n",
      "movies\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 376601 entries, 0 to 376600\n",
      "Data columns (total 2 columns):\n",
      "subreddit    376601 non-null object\n",
      "body         376601 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 8.6+ MB\n",
      "None\n",
      "=========\n",
      "\n",
      "books\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 72193 entries, 0 to 72192\n",
      "Data columns (total 2 columns):\n",
      "subreddit    72193 non-null object\n",
      "body         72193 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.7+ MB\n",
      "None\n",
      "=========\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for (subreddit,subject) in subreddits:\n",
    "    print(subject+'\\n')\n",
    "    print(subreddit.info())\n",
    "    print('=========\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_frames = [bicycling, history, philosophy, elifive, homebrewing, askanthro, mathematics,\\\n",
    "              computerscience, food, science, movies, books]\n",
    "model_training_data = pd.concat(all_frames, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 931315 entries, 0 to 931314\n",
      "Data columns (total 2 columns):\n",
      "subreddit    931315 non-null object\n",
      "body         931315 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 21.3+ MB\n"
     ]
    }
   ],
   "source": [
    "model_training_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Takes a sentence in a comment and converts it to a list of words.\n",
    "def comment_to_wordlist(comment, remove_stopwords=False ):\n",
    "    comment = re.sub(\"[^a-zA-Z]\",\" \", comment)\n",
    "    words = comment.lower().split()\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "\n",
    "    return(words)\n",
    "\n",
    "def comment_to_sentence(comment, tokenizer, remove_stopwords=False):\n",
    "    raw_sentences = tokenizer.tokenize(comment.strip())\n",
    "    \n",
    "    sentences = []\n",
    "    for s in raw_sentences:\n",
    "        if len(s)>0:\n",
    "            sentences.append(comment_to_wordlist(s, remove_stopwords))\n",
    "    #rof\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'white', u'errywhere']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download a tokenizer to parse comments into sentences\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "comment_to_sentence(model_training_data.loc[1,'body'], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for comment in model_training_data['body']:\n",
    "    sentences += comment_to_sentence(comment, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2477632\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Word2Vec parameters\n",
    "Thanks to Kaggle for the great descriptions!\n",
    "\n",
    "- **Architecture**: Architecture options are skip-gram (default) or continuous bag of words. We found that skip-gram was very slightly slower but produced better results.\n",
    "- **Training algorithm**: Hierarchical softmax (default) or negative sampling. For us, the default worked well.\n",
    "- **Downsampling of frequent words**: The Google documentation recommends values between .00001 and .001. For us, values closer 0.001 seemed to improve the accuracy of the final model.\n",
    "- **Word vector dimensionality**: More features result in longer runtimes, and often, but not always, result in better models. Reasonable values can be in the tens to hundreds; we used 300.\n",
    "- **Context / window size**: How many words of context should the training algorithm take into account? 10 seems to work well for hierarchical softmax (more is better, up to a point).\n",
    "- **Worker threads**: Number of parallel processes to run. This is computer-specific, but between 4 and 6 should work on most systems.\n",
    "- **Minimum word count**: This helps limit the size of the vocabulary to meaningful words. Any word that does not occur at least this many times across all documents is ignored. Reasonable values could be between 10 and 100. In this case, since each movie occurs 30 times, we set the minimum word count to 40, to avoid attaching too much importance to individual movie titles. This resulted in an overall vocabulary size of around 15,000 words. Higher values also help limit run time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_features = 300\n",
    "min_word_count = [10, 30, 50, 100]\n",
    "context = [3, 5, 10]\n",
    "downsampling = 1e-5\n",
    "num_workers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with the following parameters:\n",
      "Size of feature vector: 300\n",
      "Min word count: 10\n",
      "n-gram context: 3\n",
      "Downsampling of frequent words: 1e-05\n",
      "\n",
      "Took 222.979872942 time to train model: 300features_10minwords_3context\n",
      "Model saved.\n",
      "Training model with the following parameters:\n",
      "Size of feature vector: 300\n",
      "Min word count: 10\n",
      "n-gram context: 5\n",
      "Downsampling of frequent words: 1e-05\n",
      "\n",
      "Took 235.19748807 time to train model: 300features_10minwords_5context\n",
      "Model saved.\n",
      "Training model with the following parameters:\n",
      "Size of feature vector: 300\n",
      "Min word count: 10\n",
      "n-gram context: 10\n",
      "Downsampling of frequent words: 1e-05\n",
      "\n",
      "Took 259.743896008 time to train model: 300features_10minwords_10context\n",
      "Model saved.\n",
      "Training model with the following parameters:\n",
      "Size of feature vector: 300\n",
      "Min word count: 30\n",
      "n-gram context: 3\n",
      "Downsampling of frequent words: 1e-05\n",
      "\n",
      "Took 215.342049837 time to train model: 300features_30minwords_3context\n",
      "Model saved.\n",
      "Training model with the following parameters:\n",
      "Size of feature vector: 300\n",
      "Min word count: 30\n",
      "n-gram context: 5\n",
      "Downsampling of frequent words: 1e-05\n",
      "\n",
      "Took 228.433310032 time to train model: 300features_30minwords_5context\n",
      "Model saved.\n",
      "Training model with the following parameters:\n",
      "Size of feature vector: 300\n",
      "Min word count: 30\n",
      "n-gram context: 10\n",
      "Downsampling of frequent words: 1e-05\n",
      "\n",
      "Took 239.817675829 time to train model: 300features_30minwords_10context\n",
      "Model saved.\n",
      "Training model with the following parameters:\n",
      "Size of feature vector: 300\n",
      "Min word count: 50\n",
      "n-gram context: 3\n",
      "Downsampling of frequent words: 1e-05\n",
      "\n",
      "Took 192.978592873 time to train model: 300features_50minwords_3context\n",
      "Model saved.\n",
      "Training model with the following parameters:\n",
      "Size of feature vector: 300\n",
      "Min word count: 50\n",
      "n-gram context: 5\n",
      "Downsampling of frequent words: 1e-05\n",
      "\n",
      "Took 203.529958963 time to train model: 300features_50minwords_5context\n",
      "Model saved.\n",
      "Training model with the following parameters:\n",
      "Size of feature vector: 300\n",
      "Min word count: 100\n",
      "n-gram context: 3\n",
      "Downsampling of frequent words: 1e-05\n",
      "\n",
      "Took 208.040830851 time to train model: 300features_100minwords_3context\n",
      "Model saved.\n",
      "Training model with the following parameters:\n",
      "Size of feature vector: 300\n",
      "Min word count: 100\n",
      "n-gram context: 5\n",
      "Downsampling of frequent words: 1e-05\n",
      "\n",
      "Took 214.90149498 time to train model: 300features_100minwords_5context\n",
      "Model saved.\n",
      "Training model with the following parameters:\n",
      "Size of feature vector: 300\n",
      "Min word count: 100\n",
      "n-gram context: 10\n",
      "Downsampling of frequent words: 1e-05\n",
      "\n",
      "Took 226.938553095 time to train model: 300features_100minwords_10context\n",
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "import time\n",
    "import logging\n",
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "num = 300\n",
    "\n",
    "for mwc in min_word_count:\n",
    "    for c in context:\n",
    "        # Initialize and train each model \n",
    "        print(\"Training model with the following parameters:\")\n",
    "        message = \"Size of feature vector: {}\\nMin word count: {}\\nn-gram context: {}\\nDownsampling of frequent words: {}\\n\".format(num, mwc, c, downsampling)\n",
    "        print(message)\n",
    "        \n",
    "        start = time.time();\n",
    "        model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "                    size=num, min_count = mwc, \\\n",
    "                    window = c, sample = downsampling)\n",
    "        end = time.time();\n",
    "        total = end-start;\n",
    "        # Compress, name, and store each model\n",
    "        model.init_sims(replace=True)\n",
    "        model_name = str(num) + \"features_\" + str(mwc) + \"minwords_\" + str(c) + \"context\"\n",
    "        print(\"Took \" + str(total) + \" time to train model: \" + model_name)\n",
    "        model.save(model_name)\n",
    "        print(\"Model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with the following parameters:\n",
      "Size of feature vector: 300\n",
      "Min word count: 10\n",
      "n-gram context: 3\n",
      "Downsampling of frequent words: 1e-05\n",
      "\n",
      "Took 271.326533079 time to train model: 300features_10minwords_10context\n",
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "# This cell is used for training an individual model\n",
    "from gensim.models import word2vec\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# Initialize and train each model \n",
    "print(\"Training model with the following parameters:\")\n",
    "message = \"Size of feature vector: {}\\nMin word count: {}\\nn-gram context: {}\\nDownsampling of frequent words: {}\\n\".format(300, 10, 3, downsampling)\n",
    "print(message)\n",
    "        \n",
    "start = time.time();\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=300, min_count = 10, \\\n",
    "            window = 10, sample = downsampling)\n",
    "end = time.time();\n",
    "total = end-start;\n",
    "# Compress, name, and store each model\n",
    "model.init_sims(replace=True)\n",
    "model_name = str(300) + \"features_\" + str(10) + \"minwords_\" + str(10) + \"context\"\n",
    "print(\"Took \" + str(total) + \" time to train model: \" + model_name)\n",
    "model.save(model_name)\n",
    "print(\"Model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cells from here down are a bit of testing/exploration with the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kitchen'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"man woman child kitchen\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.3183469452418421"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((model[\"man\"]-model[\"child\"])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Not sure if this model is that great, I'm thinking the context window was too large\n",
    "# Model 1:\n",
    "#model.init_sims(replace=True)\n",
    "#model_name = \"300features_30minwords_15context_includescience\"\n",
    "#model.save(model_name)\n",
    "\n",
    "# Can load the model later with Word2Vec.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_frames = [bike_training, hist_training, phil_training, elif_training, brew_training]\n",
    "training_data = pd.concat(training_frames, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 25000 entries, 0 to 24999\n",
      "Data columns (total 2 columns):\n",
      "subreddit    25000 non-null object\n",
      "body         25000 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 585.9+ KB\n"
     ]
    }
   ],
   "source": [
    "training_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_sentences = []\n",
    "for comment in training_data['body']:\n",
    "    training_sentences += comment_to_sentence(comment, tokenizer, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Filter out sentences with less than 5 words, these are likely nonsensical\n",
    "after_filter_training_sentences = filter(lambda x: len(x)>5, training_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84710\n",
      "49508\n"
     ]
    }
   ],
   "source": [
    "print(len(training_sentences))\n",
    "print(len(after_filter_training_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'm', u'sure', u'anyone', u'could', u'afford', u'bike', u'could', u'afford', u'pay', u'someone', u'clean', u'every', u'ride']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'anthropology'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [\"bicycle\",\"statistics\",\"history\",\"philosophy\",\"homebrewing\",\"anthropology\",\"explain\"]\n",
    "\n",
    "def f(word,label):\n",
    "    return abs(model.similarity(word, label))\n",
    "\n",
    "def label(sentence,labels):\n",
    "    # Initialize distance to be high\n",
    "    best_distance = 1e8\n",
    "    l = \"\"\n",
    "    for label in labels:\n",
    "        ssds = map(lambda x: f(x,label),sentence)\n",
    "        #print(\"Label: \" + label)\n",
    "        #print(ssds)\n",
    "        #average = sum(ssds)/len(ssds)\n",
    "        if min(ssds) < best_distance:\n",
    "            l = label\n",
    "            best_distance = min(ssds)\n",
    "    return l\n",
    "\n",
    "test = after_filter_training_sentences[1]\n",
    "print(test)\n",
    "label(test,labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
