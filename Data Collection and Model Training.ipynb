{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import gensim\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use the NLTK downloader to download stopwords and punkt tokenizer (for breaking paragraphs into sentences)\n",
    "\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "database.sqlite\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sql_conn = sqlite3.connect('../data/database.sqlite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Subreddits to explore\n",
    "Comments from the following subset of subreddits will be explored:\n",
    "\n",
    "- r/bicycling\n",
    "- r/Statistics\n",
    "- r/history\n",
    "- r/philosophy\n",
    "- r/Homebrewing\n",
    "- r/AskAnthropology\n",
    "- r/explainlikeimfive\n",
    "\n",
    "Some of these subreddits did not contain a sufficient number of comments to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mathematics = pd.read_sql(\"SELECT subreddit, body FROM May2015 WHERE subreddit == 'mathematics'\",sql_conn)\n",
    "computerscience = pd.read_sql(\"SELECT subreddit, body FROM May2015 WHERE subreddit == 'computerscience'\",sql_conn)\n",
    "statistics = pd.read_sql(\"SELECT subreddit, body FROM May2015 WHERE subreddit == 'statistics'\",sql_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = pd.read_sql(\"SELECT subreddit, body FROM May2015 WHERE subreddit == 'history'\",sql_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "philosophy = pd.read_sql(\"SELECT subreddit, body FROM May2015 WHERE subreddit == 'philosophy'\",sql_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "elifive = pd.read_sql(\"SELECT subreddit, body FROM May2015 WHERE subreddit == 'explainlikeimfive'\",sql_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "askanthro = pd.read_sql(\"SELECT subreddit, body FROM May2015 WHERE subreddit == 'AskAnthropology'\",sql_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "homebrewing = pd.read_sql(\"SELECT subreddit, body FROM May2015 WHERE subreddit == 'Homebrewing'\",sql_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bicycling = pd.read_sql(\"SELECT subreddit, body FROM May2015 WHERE subreddit == 'bicycling'\", sql_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "food = pd.read_sql(\"SELECT subreddit, body FROM May2015 WHERE subreddit == 'food'\", sql_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gaming = pd.read_sql(\"SELECT subreddit, body FROM May2015 WHERE subreddit == 'gaming'\", sql_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "politics = pd.read_sql(\"SELECT subreddit, body FROM May2015 WHERE subreddit == 'politics'\", sql_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Array of tuples, with df and nick\n",
    "subreddits = [(bicycling,'bike'),(history,'hist'),(philosophy,'phil'),\n",
    "              (elifive,'elif'),(homebrewing,'homebrew'),(askanthro,'anthro'),\n",
    "              (mathematics,'math'),(computerscience,'cs'),(food,\"food\"),\n",
    "              (gaming,\"gaming\"),(politics,\"politics\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bike\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 40111 entries, 0 to 40110\n",
      "Data columns (total 2 columns):\n",
      "subreddit    40111 non-null object\n",
      "body         40111 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 940.1+ KB\n",
      "None\n",
      "=========\n",
      "\n",
      "hist\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 25242 entries, 0 to 25241\n",
      "Data columns (total 2 columns):\n",
      "subreddit    25242 non-null object\n",
      "body         25242 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 591.6+ KB\n",
      "None\n",
      "=========\n",
      "\n",
      "phil\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 16943 entries, 0 to 16942\n",
      "Data columns (total 2 columns):\n",
      "subreddit    16943 non-null object\n",
      "body         16943 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 397.1+ KB\n",
      "None\n",
      "=========\n",
      "\n",
      "elif\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 223148 entries, 0 to 223147\n",
      "Data columns (total 2 columns):\n",
      "subreddit    223148 non-null object\n",
      "body         223148 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 5.1+ MB\n",
      "None\n",
      "=========\n",
      "\n",
      "homebrew\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 29948 entries, 0 to 29947\n",
      "Data columns (total 2 columns):\n",
      "subreddit    29948 non-null object\n",
      "body         29948 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 701.9+ KB\n",
      "None\n",
      "=========\n",
      "\n",
      "anthro\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1624 entries, 0 to 1623\n",
      "Data columns (total 2 columns):\n",
      "subreddit    1624 non-null object\n",
      "body         1624 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 38.1+ KB\n",
      "None\n",
      "=========\n",
      "\n",
      "math\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 150 entries, 0 to 149\n",
      "Data columns (total 2 columns):\n",
      "subreddit    150 non-null object\n",
      "body         150 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 3.5+ KB\n",
      "None\n",
      "=========\n",
      "\n",
      "cs\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 711 entries, 0 to 710\n",
      "Data columns (total 2 columns):\n",
      "subreddit    711 non-null object\n",
      "body         711 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 16.7+ KB\n",
      "None\n",
      "=========\n",
      "\n",
      "food\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 55231 entries, 0 to 55230\n",
      "Data columns (total 2 columns):\n",
      "subreddit    55231 non-null object\n",
      "body         55231 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.3+ MB\n",
      "None\n",
      "=========\n",
      "\n",
      "gaming\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 361265 entries, 0 to 361264\n",
      "Data columns (total 2 columns):\n",
      "subreddit    361265 non-null object\n",
      "body         361265 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 8.3+ MB\n",
      "None\n",
      "=========\n",
      "\n",
      "politics\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 244927 entries, 0 to 244926\n",
      "Data columns (total 2 columns):\n",
      "subreddit    244927 non-null object\n",
      "body         244927 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 5.6+ MB\n",
      "None\n",
      "=========\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for (subreddit,nick) in subreddits:\n",
    "    print(nick+'\\n')\n",
    "    print(subreddit.info())\n",
    "    print('=========\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/gcc\r\n"
     ]
    }
   ],
   "source": [
    "!which gcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Array of tuples, with df and nick\n",
    "# subreddits = [(bicycling,'bike'),(history,'hist'),(philosophy,'phil'),\n",
    "#               (elifive,'elif'),(homebrewing,'homebrew'),(askanthro,'anthro'),\n",
    "#               (mathematics,'math'),(computerscience,'cs')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bike_training = bicycling[:5000]\n",
    "hist_training = history[:5000]\n",
    "phil_training = philosophy[:5000]\n",
    "elif_training = elifive[:5000]\n",
    "brew_training = homebrewing[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_frames = [bicycling, history, philosophy, elifive, homebrewing, askanthro, mathematics,\\\n",
    "              computerscience, food, gaming, politics]\n",
    "model_training_data = pd.concat(all_frames, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 999300 entries, 0 to 999299\n",
      "Data columns (total 2 columns):\n",
      "subreddit    999300 non-null object\n",
      "body         999300 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 22.9+ MB\n"
     ]
    }
   ],
   "source": [
    "model_training_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Takes a sentence in a comment and converts it to a list of words.\n",
    "def comment_to_wordlist(comment, remove_stopwords=False ):\n",
    "    comment = re.sub(\"[^a-zA-Z]\",\" \", comment)\n",
    "    words = comment.lower().split()\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def comment_to_sentence(comment, tokenizer, remove_stopwords=False):\n",
    "    raw_sentences = tokenizer.tokenize(comment.strip())\n",
    "    \n",
    "    sentences = []\n",
    "    for s in raw_sentences:\n",
    "        if len(s)>0:\n",
    "            sentences.append(comment_to_wordlist(s, remove_stopwords))\n",
    "    #rof\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'white', u'errywhere']]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download a tokenizer to parse comments into sentences\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "comment_to_sentence(model_training_data.loc[1,'body'], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for comment in model_training_data['body']:\n",
    "    sentences += comment_to_sentence(comment, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2645135"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Word2Vec parameters\n",
    "Thanks to Kaggle for the great descriptions!\n",
    "\n",
    "- **Architecture**: Architecture options are skip-gram (default) or continuous bag of words. We found that skip-gram was very slightly slower but produced better results.\n",
    "- **Training algorithm**: Hierarchical softmax (default) or negative sampling. For us, the default worked well.\n",
    "- **Downsampling of frequent words**: The Google documentation recommends values between .00001 and .001. For us, values closer 0.001 seemed to improve the accuracy of the final model.\n",
    "- **Word vector dimensionality**: More features result in longer runtimes, and often, but not always, result in better models. Reasonable values can be in the tens to hundreds; we used 300.\n",
    "- **Context / window size**: How many words of context should the training algorithm take into account? 10 seems to work well for hierarchical softmax (more is better, up to a point).\n",
    "- **Worker threads**: Number of parallel processes to run. This is computer-specific, but between 4 and 6 should work on most systems.\n",
    "- **Minimum word count**: This helps limit the size of the vocabulary to meaningful words. Any word that does not occur at least this many times across all documents is ignored. Reasonable values could be between 10 and 100. In this case, since each movie occurs 30 times, we set the minimum word count to 40, to avoid attaching too much importance to individual movie titles. This resulted in an overall vocabulary size of around 15,000 words. Higher values also help limit run time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with the following parameters:\n",
      "Size of feature vector: 300\n",
      "Min word count: 10\n",
      "n-gram context: 3\n",
      "Downsampling of frequent words: 1e-05\n",
      "\n",
      "Took 236.530112028 time to train model: 300features_10minwords_3context\n",
      "Model saved.\n",
      "Training model with the following parameters:\n",
      "Size of feature vector: 300\n",
      "Min word count: 10\n",
      "n-gram context: 5\n",
      "Downsampling of frequent words: 1e-05\n",
      "\n",
      "Took 250.579326868 time to train model: 300features_10minwords_5context\n",
      "Model saved.\n",
      "Training model with the following parameters:\n",
      "Size of feature vector: 300\n",
      "Min word count: 10\n",
      "n-gram context: 10\n",
      "Downsampling of frequent words: 1e-05\n",
      "\n",
      "Took 273.785542011 time to train model: 300features_10minwords_10context\n",
      "Model saved.\n",
      "Training model with the following parameters:\n",
      "Size of feature vector: 300\n",
      "Min word count: 20\n",
      "n-gram context: 3\n",
      "Downsampling of frequent words: 1e-05\n",
      "\n",
      "Took 232.727242947 time to train model: 300features_20minwords_3context\n",
      "Model saved.\n",
      "Training model with the following parameters:\n",
      "Size of feature vector: 300\n",
      "Min word count: 20\n",
      "n-gram context: 5\n",
      "Downsampling of frequent words: 1e-05\n",
      "\n",
      "Took 243.728968143 time to train model: 300features_20minwords_5context\n",
      "Model saved.\n",
      "Training model with the following parameters:\n",
      "Size of feature vector: 300\n",
      "Min word count: 20\n",
      "n-gram context: 10\n",
      "Downsampling of frequent words: 1e-05\n",
      "\n",
      "Took 266.286294222 time to train model: 300features_20minwords_10context\n",
      "Model saved.\n",
      "Training model with the following parameters:\n",
      "Size of feature vector: 300\n",
      "Min word count: 30\n",
      "n-gram context: 3\n",
      "Downsampling of frequent words: 1e-05\n",
      "\n",
      "Took 231.487907887 time to train model: 300features_30minwords_3context\n",
      "Model saved.\n",
      "Training model with the following parameters:\n",
      "Size of feature vector: 300\n",
      "Min word count: 30\n",
      "n-gram context: 5\n",
      "Downsampling of frequent words: 1e-05\n",
      "\n",
      "Took 242.156858921 time to train model: 300features_30minwords_5context\n",
      "Model saved.\n",
      "Training model with the following parameters:\n",
      "Size of feature vector: 300\n",
      "Min word count: 30\n",
      "n-gram context: 10\n",
      "Downsampling of frequent words: 1e-05\n",
      "\n",
      "Took 260.653708935 time to train model: 300features_30minwords_10context\n",
      "Model saved.\n",
      "Training model with the following parameters:\n",
      "Size of feature vector: 300\n",
      "Min word count: 40\n",
      "n-gram context: 3\n",
      "Downsampling of frequent words: 1e-05\n",
      "\n",
      "Took 231.112336874 time to train model: 300features_40minwords_3context\n",
      "Model saved.\n",
      "Training model with the following parameters:\n",
      "Size of feature vector: 300\n",
      "Min word count: 40\n",
      "n-gram context: 5\n",
      "Downsampling of frequent words: 1e-05\n",
      "\n",
      "Took 238.961984873 time to train model: 300features_40minwords_5context\n",
      "Model saved.\n",
      "Training model with the following parameters:\n",
      "Size of feature vector: 300\n",
      "Min word count: 40\n",
      "n-gram context: 10\n",
      "Downsampling of frequent words: 1e-05\n",
      "\n",
      "Took 256.607560873 time to train model: 300features_40minwords_10context\n",
      "Model saved.\n",
      "Training model with the following parameters:\n",
      "Size of feature vector: 300\n",
      "Min word count: 50\n",
      "n-gram context: 3\n",
      "Downsampling of frequent words: 1e-05\n",
      "\n",
      "Took 228.439174891 time to train model: 300features_50minwords_3context\n",
      "Model saved.\n",
      "Training model with the following parameters:\n",
      "Size of feature vector: 300\n",
      "Min word count: 50\n",
      "n-gram context: 5\n",
      "Downsampling of frequent words: 1e-05\n",
      "\n",
      "Took 236.30133009 time to train model: 300features_50minwords_5context\n",
      "Model saved.\n",
      "Training model with the following parameters:\n",
      "Size of feature vector: 300\n",
      "Min word count: 50\n",
      "n-gram context: 10\n",
      "Downsampling of frequent words: 1e-05\n",
      "\n",
      "Took 253.508800983 time to train model: 300features_50minwords_10context\n",
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "import time\n",
    "import logging\n",
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "num_features = [300, 400, 500]\n",
    "min_word_count = [10, 20, 30, 40, 50]\n",
    "context = [3, 5, 10]\n",
    "downsampling = 1e-5\n",
    "num_workers = 4\n",
    "\n",
    "num = 300\n",
    "\n",
    "for mwc in min_word_count:\n",
    "    for c in context:\n",
    "        # Initialize and train each model \n",
    "        print(\"Training model with the following parameters:\")\n",
    "        message = \"Size of feature vector: {}\\nMin word count: {}\\nn-gram context: {}\\nDownsampling of frequent words: {}\\n\".format(num, mwc, c, downsampling)\n",
    "        print(message)\n",
    "        \n",
    "        start = time.time();\n",
    "        model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "                    size=num, min_count = mwc, \\\n",
    "                    window = c, sample = downsampling)\n",
    "        end = time.time();\n",
    "        total = end-start;\n",
    "        # Compress, name, and store each model\n",
    "        model.init_sims(replace=True)\n",
    "        model_name = str(num) + \"features_\" + str(mwc) + \"minwords_\" + str(c) + \"context\"\n",
    "        print(\"Took \" + str(total) + \" time to train model: \" + model_name)\n",
    "        model.save(model_name)\n",
    "        print(\"Model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num = 400\n",
    "\n",
    "for mwc in min_word_count:\n",
    "    for c in context:\n",
    "        # Initialize and train each model\n",
    "            \n",
    "        print(\"Training model with the following parameters:\")\n",
    "        message = \"Size of feature vector: {}\\nMin word count: {}\\nn-gram context: {}\\nDownsampling of frequent words: {}\\n\".format(num, mwc, c, downsampling)\n",
    "        print(message)\n",
    "        model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "                    size=num, min_count = mwc, \\\n",
    "                    window = c, sample = downsampling)\n",
    "            \n",
    "        # Compress, name, and store each model\n",
    "        model.init_sims(replace=True)\n",
    "        model_name = str(num) + \"features_\" + str(mwc) + \"minwords_\" + str(c) + \"context\"\n",
    "        model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kitchen'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"man woman child kitchen\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.3183469452418421"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((model[\"man\"]-model[\"child\"])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Not sure if this model is that great, I'm thinking the context window was too large\n",
    "# Model 1:\n",
    "#model.init_sims(replace=True)\n",
    "#model_name = \"300features_30minwords_15context_includescience\"\n",
    "#model.save(model_name)\n",
    "\n",
    "# Can load the model later with Word2Vec.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_frames = [bike_training, hist_training, phil_training, elif_training, brew_training]\n",
    "training_data = pd.concat(training_frames, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 25000 entries, 0 to 24999\n",
      "Data columns (total 2 columns):\n",
      "subreddit    25000 non-null object\n",
      "body         25000 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 585.9+ KB\n"
     ]
    }
   ],
   "source": [
    "training_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_sentences = []\n",
    "for comment in training_data['body']:\n",
    "    training_sentences += comment_to_sentence(comment, tokenizer, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Filter out sentences with less than 5 words, these are likely nonsensical\n",
    "after_filter_training_sentences = filter(lambda x: len(x)>5, training_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84710\n",
      "49508\n"
     ]
    }
   ],
   "source": [
    "print(len(training_sentences))\n",
    "print(len(after_filter_training_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations with model 1\n",
    "\n",
    "I observe that with a model consisting of:\n",
    "- 300 features\n",
    "- Minimum word count of 30\n",
    "- Context window of size 15\n",
    "- Downsampling frequent words by 0.001\n",
    "\n",
    "I find that the model has a difficult time classifying a sentence to the proper label. From the ground truth, I know it comes from r/bicycling. However, it appears to be more closely related to other labels. This sentence appears to have fairly common words, but \"clean,\" \"ride,\" and \"bike\" should certainly be labeled \"bicycling.\"\n",
    "\n",
    "### Observations with model 2\n",
    "\n",
    "I predict that a model with these parameters may work better:\n",
    "\n",
    "I observe that with a model consisting of:\n",
    "- 300 features\n",
    "- Minimum word count of 10\n",
    "- Context window of size 5\n",
    "- Downsampling frequent words by 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'm',\n",
       " u'sure',\n",
       " u'anyone',\n",
       " u'could',\n",
       " u'afford',\n",
       " u'bike',\n",
       " u'could',\n",
       " u'afford',\n",
       " u'pay',\n",
       " u'someone',\n",
       " u'clean',\n",
       " u'every',\n",
       " u'ride']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observation 1:\n",
    "after_filter_training_sentences[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- r/bicycling\n",
    "- r/Statistics\n",
    "- r/history\n",
    "- r/philosophy\n",
    "- r/Homebrewing\n",
    "- r/AskAnthropology\n",
    "- r/explainlikeimfive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'm', u'sure', u'anyone', u'could', u'afford', u'bike', u'could', u'afford', u'pay', u'someone', u'clean', u'every', u'ride']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'anthropology'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [\"bicycle\",\"statistics\",\"history\",\"philosophy\",\"homebrewing\",\"anthropology\",\"explain\"]\n",
    "\n",
    "def f(word,label):\n",
    "    return abs(model.similarity(word, label))\n",
    "\n",
    "def label(sentence,labels):\n",
    "    # Initialize distance to be high\n",
    "    best_distance = 1e8\n",
    "    l = \"\"\n",
    "    for label in labels:\n",
    "        ssds = map(lambda x: f(x,label),sentence)\n",
    "        #print(\"Label: \" + label)\n",
    "        #print(ssds)\n",
    "        #average = sum(ssds)/len(ssds)\n",
    "        if min(ssds) < best_distance:\n",
    "            l = label\n",
    "            best_distance = min(ssds)\n",
    "    return l\n",
    "\n",
    "test = after_filter_training_sentences[1]\n",
    "print(test)\n",
    "label(test,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'rides', 0.6077573895454407),\n",
       " (u'bike', 0.5489786863327026),\n",
       " (u'commute', 0.5477386713027954)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=[\"ride\"],topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
