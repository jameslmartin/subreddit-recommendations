{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import gensim\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sql_conn = sqlite3.connect('../data/database.sql')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mathematics = pd.read_sql(\"SELECT subreddit, body FROM May2015 WHERE subreddit == 'mathematics'\",sql_conn)\n",
    "\n",
    "computerscience = pd.read_sql(\"SELECT subreddit, body FROM May2015 WHERE subreddit == 'computerscience'\",sql_conn)\n",
    "\n",
    "history = pd.read_sql(\"SELECT subreddit, body FROM May2015 WHERE subreddit == 'history'\",sql_conn)\n",
    "\n",
    "philosophy = pd.read_sql(\"SELECT subreddit, body FROM May2015 WHERE subreddit == 'philosophy'\",sql_conn)\n",
    "\n",
    "elifive = pd.read_sql(\"SELECT subreddit, body FROM May2015 WHERE subreddit == 'explainlikeimfive'\",sql_conn)\n",
    "\n",
    "askanthro = pd.read_sql(\"SELECT subreddit, body FROM May2015 WHERE subreddit == 'AskAnthropology'\",sql_conn)\n",
    "\n",
    "homebrewing = pd.read_sql(\"SELECT subreddit, body FROM May2015 WHERE subreddit == 'Homebrewing'\",sql_conn)\n",
    "\n",
    "bicycling = pd.read_sql(\"SELECT subreddit, body FROM May2015 WHERE subreddit == 'bicycling'\", sql_conn)\n",
    "\n",
    "food = pd.read_sql(\"SELECT subreddit, body FROM May2015 WHERE subreddit == 'food'\", sql_conn)\n",
    "\n",
    "science = pd.read_sql(\"SELECT subreddit, body FROM May2015 WHERE subreddit == 'science'\", sql_conn)\n",
    "\n",
    "movies = pd.read_sql(\"SELECT subreddit, body FROM May2015 WHERE subreddit == 'movies'\", sql_conn)\n",
    "\n",
    "books = pd.read_sql(\"SELECT subreddit, body FROM May2015 WHERE subreddit == 'books'\", sql_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Array of tuples, with df and subject\n",
    "subreddits = [(bicycling,'bicycling'),(history,'history'),(philosophy,'philosophy'),\n",
    "              (elifive,'explain'),(homebrewing,'homebrew'),(askanthro,'anthropology'),\n",
    "              (mathematics,'mathematics'),(computerscience,'computer'),\n",
    "              (food,\"food\"),(science,\"science\"),(movies,\"movies\"),(books,\"books\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 931315 entries, 0 to 931314\n",
      "Data columns (total 2 columns):\n",
      "subreddit    931315 non-null object\n",
      "body         931315 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 21.3+ MB\n"
     ]
    }
   ],
   "source": [
    "all_frames = [bicycling, history, philosophy, elifive, homebrewing, askanthro, mathematics,\\\n",
    "              computerscience, food, science, movies, books]\n",
    "all_data = pd.concat(all_frames, ignore_index=True)\n",
    "all_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Takes a sentence in a comment and converts it to a list of words.\n",
    "def comment_to_wordlist(comment, remove_stopwords=False ):\n",
    "    comment = re.sub(\"[^a-zA-Z]\",\" \", comment)\n",
    "    words = comment.lower().split()\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "\n",
    "    return(words)\n",
    "\n",
    "# Takes a comment and converts it to an array of sentences\n",
    "def comment_to_sentence(comment, tokenizer, remove_stopwords=False):\n",
    "    raw_sentences = tokenizer.tokenize(comment.strip())\n",
    "    \n",
    "    sentences = []\n",
    "    for s in raw_sentences:\n",
    "        if len(s)>0:\n",
    "            sentences.append(comment_to_wordlist(s, remove_stopwords))\n",
    "    return sentences\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "comments = []\n",
    "for i in range(len(all_data)):\n",
    "    comments.append(comment_to_wordlist(all_data.iloc[i]['body'], tokenizer))\n",
    "\n",
    "ground_truth_labels = all_data['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store comments, as the previous cell takes some time to run\n",
    "import pickle\n",
    "\n",
    "output = open('comments.pkl', 'wb')\n",
    "pickle.dump(comments, output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reduce_comments(words, model):\n",
    "    # Pre-allocate average vector representation\n",
    "    average = np.zeros(300, dtype=np.float64)\n",
    "    in_vocab = 0\n",
    "    for word in words:\n",
    "        if word in model.vocab:\n",
    "            in_vocab += 1\n",
    "            average = average + np.array(model[word], dtype=float)\n",
    "    return np.divide(average,in_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "m = \"300features_10minwords_10context\"\n",
    "current_model = word2vec.Word2Vec.load('new_models/' + m);\n",
    "\n",
    "single_rep_comments = map(lambda comment: reduce_comments(comment, current_model), comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "931315"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(single_rep_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check to make sure we have a 1x300 vector representation of each *comment*\n",
    "for c in single_rep_comments:\n",
    "    nans = 0\n",
    "    if not len(c) == 300:\n",
    "        print len(c)\n",
    "    if True in np.isnan(c):\n",
    "        nans += 1\n",
    "# No output is good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert our vector representation to a nice numpy matrix\n",
    "d = np.asarray(single_rep_comments, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(931315, 300)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pull out indices of rows where the vector representation does not contain a NaN\n",
    "# Not sure how NaNs crept in here, possibly due to overflow issues\n",
    "ix_non_nans = np.where(~(np.isnan(d).any(axis=1)))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Filter out NaNs on comments and labels\n",
    "d = d[ix_non_nans]\n",
    "gtl = np.asarray(ground_truth_labels)[ix_non_nans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(copy_x=True, init='k-means++', max_iter=300, n_clusters=12, n_init=10,\n",
       "    n_jobs=1, precompute_distances='auto', random_state=None, tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cluster using KMeans, initializing k = 12 (one for each of the subreddits we drew from)\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters = 12)\n",
    "kmeans.fit(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "centroids = kmeans.cluster_centers_\n",
    "centroids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pickle/store centroids\n",
    "output = open('centroids.pkl', 'wb')\n",
    "pickle.dump(centroids, output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load centroids back in:\n",
    "#pkl_file = open('centroids.pkl','rb')\n",
    "#centroids = pickle.load(pkl_file)\n",
    "#pkl_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'storyteller', 0.8631452322006226),\n",
       " (u'heroines', 0.8625438809394836),\n",
       " (u'toho', 0.8603470325469971),\n",
       " (u'worldbuilding', 0.8569574356079102),\n",
       " (u'dcau', 0.856163501739502)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list(centroids[0])\n",
    "current_model.most_similar(positive=[centroids[11]], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure labels and d are the same length\n",
    "len(d) == len(gtl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load back in:\n",
    "#pkl_file = open('comments.pkl','rb')\n",
    "#comments = pickle.load(pkl_file)\n",
    "#pkl_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run PCA and save\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = 3)\n",
    "red_svd_src = pca.fit_transform(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfdict = {'x': red_svd_src[:,0],\n",
    "          'y': red_svd_src[:,1],\n",
    "          'z': red_svd_src[:,2],\n",
    "          'subreddit': gtl}\n",
    "\n",
    "df = pd.DataFrame(dfdict)\n",
    "df.to_csv('reduced_data_3.csv',encoding='utf8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
